# Utility per la visualizzazione dei risultati

# Import condizionali
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    # Mock per numpy functions
    class MockNumpy:
        @staticmethod
        def isnan(value):
            try:
                return value != value  # NaN check senza numpy
            except:
                return False
        
        @staticmethod
        def mean(values):
            return sum(values) / len(values) if values else 0
        
        @staticmethod
        def median(values):
            sorted_values = sorted(values)
            n = len(sorted_values)
            if n == 0:
                return 0
            elif n % 2 == 0:
                return (sorted_values[n//2-1] + sorted_values[n//2]) / 2
            else:
                return sorted_values[n//2]
        
        @staticmethod
        def max(values):
            return max(values) if values else 0
        
        @staticmethod
        def min(values):
            return min(values) if values else 0
    
    np = MockNumpy()


class DisplayHelper:
    """Classe per la visualizzazione dei risultati"""
    
    @staticmethod
    def get_status_emoji(score):
        """Restituisce emoji basato sul punteggio"""
        if score >= 0.8:
            return "üü¢"  # Eccellente
        elif score >= 0.6:
            return "‚úÖ"  # Buono
        elif score >= 0.4:
            return "‚ö†Ô∏è"  # Discreto
        else:
            return "‚ùå"  # Scarso

    @staticmethod
    def display_comprehensive_results(ragas_results, custom_results):
        """Visualizza risultati con debug esteso"""
        print("\n" + "="*80)
        print("üìä RISULTATI VALUTAZIONE RAG COMPLETA")
        print("="*80)

        # Categorizza metriche RAGAS
        metric_categories = {
            "üéØ Core RAG Metrics": ["faithfulness", "answer_relevancy", "context_precision", "context_recall"],
            "üìù Answer Quality": ["answer_correctness", "answer_similarity"],
            "üìÑ Context Analysis": ["context_entity_recall"],
            "üó£Ô∏è Language Quality": ["coherence", "fluency", "conciseness"],
            "üõ°Ô∏è Safety & Ethics": ["harmfulness", "maliciousness"],
            "üî¨ Specialized": ["summarization_score", "aspect_critique"]
        }

        # Mostra metriche RAGAS per categoria
        ragas_scores = []
        zero_scores = []

        print(f"\nüìä SEZIONE RAGAS METRICS:")
        if ragas_results and len(ragas_results) > 0:
            for category_name, metrics in metric_categories.items():
                category_metrics = {k: v for k, v in ragas_results.items() if k in metrics}
                if category_metrics:
                    print(f"\n{category_name}:")
                    for metric, value in category_metrics.items():
                        if not np.isnan(value):
                            status = DisplayHelper.get_status_emoji(value)
                            print(f"  {status} {metric:20}: {value:.4f}")
                            ragas_scores.append(value)
                            if value == 0:
                                zero_scores.append(metric)
                        else:
                            print(f"  üî∂ {metric:20}: NaN")
        else:
            print("  ‚ùå Nessuna metrica RAGAS disponibile")

        # Mostra metriche custom
        print(f"\nüîß SEZIONE CUSTOM METRICS:")
        custom_scores = []
        if custom_results and isinstance(custom_results, dict) and len(custom_results) > 0:
            print(f"‚úÖ Mostrando {len(custom_results)} metriche custom:")
            for metric, value in custom_results.items():
                try:
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        status = DisplayHelper.get_status_emoji(value)
                        print(f"  {status} {metric:25}: {value:.4f}")
                        custom_scores.append(value)
                    else:
                        print(f"  ‚ö†Ô∏è {metric:25}: {value} (invalid)")
                except Exception as e:
                    print(f"  ‚ùå Errore processando {metric}: {e}")
        else:
            print("  ‚ùå Nessuna metrica custom disponibile")

        # Statistiche globali
        all_scores = ragas_scores + custom_scores

        print(f"\nüìà STATISTICHE GLOBALI:")
        print(f"  üìä RAGAS scores: {len(ragas_scores)}")
        print(f"  üîß Custom scores: {len(custom_scores)}")
        print(f"  üìã Total scores: {len(all_scores)}")

        if all_scores:
            avg_score = np.mean(all_scores)
            median_score = np.median(all_scores)
            max_score = np.max(all_scores)
            min_score = np.min(all_scores)

            print(f"  üìä Score medio:      {avg_score:.4f}")
            print(f"  üéØ Score mediano:    {median_score:.4f}")
            print(f"  üèÜ Score massimo:    {max_score:.4f}")
            print(f"  ‚ö†Ô∏è Score minimo:     {min_score:.4f}")

            # Rating globale
            if avg_score >= 0.8:
                rating = "üèÜ ECCELLENTE"
            elif avg_score >= 0.6:
                rating = "‚úÖ BUONO"
            elif avg_score >= 0.4:
                rating = "‚ö†Ô∏è DISCRETO"
            else:
                rating = "‚ùå NECESSITA MIGLIORAMENTI"

            print(f"  üéñÔ∏è Rating globale:   {rating}")

            # Aree di miglioramento
            if zero_scores:
                print(f"\nüîß AREE DI MIGLIORAMENTO:")
                for metric in zero_scores:
                    print(f"  ‚ùå {metric}")
        else:
            print("  ‚ùå Nessun punteggio disponibile per le statistiche")

        print("\n" + "="*80)