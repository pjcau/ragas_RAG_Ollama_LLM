# Utility for displaying results

# Conditional imports
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    # Mock for numpy functions

    class MockNumpy:
        @staticmethod
        def isnan(value):
            try:
                return value != value  # NaN check without numpy
            except:
                return False

        @staticmethod
        def mean(values):
            return sum(values) / len(values) if values else 0

        @staticmethod
        def median(values):
            sorted_values = sorted(values)
            n = len(sorted_values)
            if n == 0:
                return 0
            elif n % 2 == 0:
                return (sorted_values[n//2-1] + sorted_values[n//2]) / 2
            else:
                return sorted_values[n//2]

        @staticmethod
        def max(values):
            return max(values) if values else 0

        @staticmethod
        def min(values):
            return min(values) if values else 0

    np = MockNumpy()


class DisplayHelper:
    """Class for displaying results"""

    @staticmethod
    def get_status_emoji(score):
        """Returns emoji based on score"""
        if score >= 0.8:
            return "ğŸŸ¢"  # Excellent
        elif score >= 0.6:
            return "âœ…"  # Good
        elif score >= 0.4:
            return "âš ï¸"  # Fair
        else:
            return "âŒ"  # Poor

    @staticmethod
    def display_comprehensive_results(ragas_results, custom_results):
        """Displays results with extended debug"""
        print("\n" + "="*80)
        print("ğŸ“Š COMPREHENSIVE RAG EVALUATION RESULTS")
        print("="*80)

        # Categorize RAGAS metrics
        metric_categories = {
            "ğŸ¯ Core RAG Metrics": ["faithfulness", "answer_relevancy", "context_precision", "context_recall"],
            "ğŸ“ Answer Quality": ["answer_correctness", "answer_similarity"],
            "ğŸ“„ Context Analysis": ["context_entity_recall"],
            "ğŸ—£ï¸ Language Quality": ["coherence", "fluency", "conciseness"],
            "ğŸ›¡ï¸ Safety & Ethics": ["harmfulness", "maliciousness"],
            "ğŸ”¬ Specialized": ["summarization_score", "aspect_critique"]
        }

        # Show RAGAS metrics by category
        ragas_scores = []
        zero_scores = []

        print(f"\nğŸ“Š RAGAS METRICS SECTION:")
        if ragas_results and len(ragas_results) > 0:
            for category_name, metrics in metric_categories.items():
                category_metrics = {k: v for k,
                                    v in ragas_results.items() if k in metrics}
                if category_metrics:
                    print(f"\n{category_name}:")
                    for metric, value in category_metrics.items():
                        if not np.isnan(value):
                            status = DisplayHelper.get_status_emoji(value)
                            print(f"  {status} {metric:20}: {value:.4f}")
                            ragas_scores.append(value)
                            if value == 0:
                                zero_scores.append(metric)
                        else:
                            print(f"  ğŸ”¶ {metric:20}: NaN")
        else:
            print("  âŒ No RAGAS metrics available")

        # Show custom metrics
        print(f"\nğŸ”§ CUSTOM METRICS SECTION:")
        custom_scores = []
        if custom_results and isinstance(custom_results, dict) and len(custom_results) > 0:
            print(f"âœ… Showing {len(custom_results)} custom metrics:")
            for metric, value in custom_results.items():
                try:
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        status = DisplayHelper.get_status_emoji(value)
                        print(f"  {status} {metric:25}: {value:.4f}")
                        custom_scores.append(value)
                    else:
                        print(f"  âš ï¸ {metric:25}: {value} (invalid)")
                except Exception as e:
                    print(f"  âŒ Error processing {metric}: {e}")
        else:
            print("  âŒ No custom metrics available")

        # Global statistics
        all_scores = ragas_scores + custom_scores

        print(f"\nğŸ“ˆ GLOBAL STATISTICS:")
        print(f"  ğŸ“Š RAGAS scores: {len(ragas_scores)}")
        print(f"  ğŸ”§ Custom scores: {len(custom_scores)}")
        print(f"  ğŸ“‹ Total scores: {len(all_scores)}")

        if all_scores:
            avg_score = np.mean(all_scores)
            median_score = np.median(all_scores)
            max_score = np.max(all_scores)
            min_score = np.min(all_scores)

            print(f"  ğŸ“Š Average score:    {avg_score:.4f}")
            print(f"  ğŸ¯ Median score:     {median_score:.4f}")
            print(f"  ğŸ† Maximum score:    {max_score:.4f}")
            print(f"  âš ï¸ Minimum score:    {min_score:.4f}")

            # Global rating
            if avg_score >= 0.8:
                rating = "ğŸ† EXCELLENT"
            elif avg_score >= 0.6:
                rating = "âœ… GOOD"
            elif avg_score >= 0.4:
                rating = "âš ï¸ FAIR"
            else:
                rating = "âŒ NEEDS IMPROVEMENT"

            print(f"  ğŸ–ï¸ Global rating:    {rating}")

            # Areas for improvement
            if zero_scores:
                print(f"\nğŸ”§ AREAS FOR IMPROVEMENT:")
                for metric in zero_scores:
                    print(f"  âŒ {metric}")
        else:
            print("  âŒ No scores available for statistics")

        print("\n" + "="*80)

    @staticmethod
    def format_comprehensive_results(ragas_results, custom_results):
        """Formats results with extended debug and returns a string"""
        output = []

        output.append("=" * 80)
        output.append("ğŸ“Š COMPREHENSIVE RAG EVALUATION RESULTS")
        output.append("=" * 80)

        # Categorize RAGAS metrics
        metric_categories = {
            "ğŸ¯ Core RAG Metrics": ["faithfulness", "answer_relevancy", "context_precision", "context_recall"],
            "ğŸ“ Answer Quality": ["answer_correctness", "answer_similarity"],
            "ğŸ“„ Context Analysis": ["context_entity_recall"],
            "ğŸ—£ï¸ Language Quality": ["coherence", "fluency", "conciseness"],
            "ğŸ›¡ï¸ Safety & Ethics": ["harmfulness", "maliciousness"],
            "ğŸ”¬ Specialized": ["summarization_score", "aspect_critique"]
        }

        # Show RAGAS metrics by category
        ragas_scores = []
        zero_scores = []

        output.append("\nğŸ“Š RAGAS METRICS SECTION:")
        if ragas_results and len(ragas_results) > 0:
            for category_name, metrics in metric_categories.items():
                category_metrics = {
                    k: v for k, v in ragas_results.items() if k in metrics}
                if category_metrics:
                    output.append(f"\n{category_name}:")
                    for metric, value in category_metrics.items():
                        if not np.isnan(value):
                            status = DisplayHelper.get_status_emoji(value)
                            output.append(
                                f"  {status} {metric:20}: {value:.4f}")
                            ragas_scores.append(value)
                            if value == 0:
                                zero_scores.append(metric)
                        else:
                            output.append(f"  ğŸ”¶ {metric:20}: NaN")
        else:
            output.append("  âŒ No RAGAS metrics available")

        # Show custom metrics
        output.append(f"\nğŸ”§ CUSTOM METRICS SECTION:")
        custom_scores = []
        if custom_results and isinstance(custom_results, dict) and len(custom_results) > 0:
            output.append(
                f"âœ… Showing {len(custom_results)} custom metrics:")
            for metric, value in custom_results.items():
                try:
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        status = DisplayHelper.get_status_emoji(value)
                        output.append(
                            f"  {status} {metric:25}: {value:.4f}")
                        custom_scores.append(value)
                    else:
                        output.append(
                            f"  âš ï¸ {metric:25}: {value} (invalid)")
                except Exception as e:
                    output.append(f"  âŒ Error processing {metric}: {e}")
        else:
            output.append("  âŒ No custom metrics available")

        # Global statistics
        all_scores = ragas_scores + custom_scores

        output.append(f"\nğŸ“ˆ GLOBAL STATISTICS:")
        output.append(f"  ğŸ“Š RAGAS scores: {len(ragas_scores)}")
        output.append(f"  ğŸ”§ Custom scores: {len(custom_scores)}")
        output.append(f"  ğŸ“‹ Total scores: {len(all_scores)}")

        if all_scores:
            avg_score = np.mean(all_scores)
            median_score = np.median(all_scores)
            max_score = np.max(all_scores)
            min_score = np.min(all_scores)

            output.append(f"  ğŸ“Š Average score:    {avg_score:.4f}")
            output.append(f"  ğŸ¯ Median score:     {median_score:.4f}")
            output.append(f"  ğŸ† Maximum score:    {max_score:.4f}")
            output.append(f"  âš ï¸ Minimum score:    {min_score:.4f}")

            # Global rating
            if avg_score >= 0.8:
                rating = "ğŸ† EXCELLENT"
            elif avg_score >= 0.6:
                rating = "âœ… GOOD"
            elif avg_score >= 0.4:
                rating = "âš ï¸ FAIR"
            else:
                rating = "âŒ NEEDS IMPROVEMENT"

            output.append(f"  ğŸ–ï¸ Global rating:    {rating}")

            # Areas for improvement
            if zero_scores:
                output.append(f"\nğŸ”§ AREAS FOR IMPROVEMENT:")
                for metric in zero_scores:
                    output.append(f"  âŒ {metric}")
        else:
            output.append(
                "  âŒ No scores available for statistics")

        output.append("\n" + "=" * 80)

        return "\n".join(output)

    @staticmethod
    def format_comprehensive_results_simple(ragas_results, custom_results):
        """Simplified version that returns only main statistics"""
        ragas_scores = [v for v in ragas_results.values() if isinstance(
            v, (int, float)) and not np.isnan(v)] if ragas_results else []
        custom_scores = [v for v in custom_results.values() if isinstance(
            v, (int, float)) and not np.isnan(v)] if custom_results else []
        all_scores = ragas_scores + custom_scores

        if not all_scores:
            return "âŒ No scores available"

        avg_score = np.mean(all_scores)

        # Rating
        if avg_score >= 0.8:
            rating = "ğŸ† EXCELLENT"
        elif avg_score >= 0.6:
            rating = "âœ… GOOD"
        elif avg_score >= 0.4:
            rating = "âš ï¸ FAIR"
        else:
            rating = "âŒ NEEDS IMPROVEMENT"

        return f"ğŸ“Š Score: {avg_score:.4f} | {rating} | RAGAS: {len(ragas_scores)} | Custom: {len(custom_scores)}"
