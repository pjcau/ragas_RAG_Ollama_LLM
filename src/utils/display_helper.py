# Utility per la visualizzazione dei risultati

# Import condizionali
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    # Mock per numpy functions

    class MockNumpy:
        @staticmethod
        def isnan(value):
            try:
                return value != value  # NaN check senza numpy
            except:
                return False

        @staticmethod
        def mean(values):
            return sum(values) / len(values) if values else 0

        @staticmethod
        def median(values):
            sorted_values = sorted(values)
            n = len(sorted_values)
            if n == 0:
                return 0
            elif n % 2 == 0:
                return (sorted_values[n//2-1] + sorted_values[n//2]) / 2
            else:
                return sorted_values[n//2]

        @staticmethod
        def max(values):
            return max(values) if values else 0

        @staticmethod
        def min(values):
            return min(values) if values else 0

    np = MockNumpy()


class DisplayHelper:
    """Classe per la visualizzazione dei risultati"""

    @staticmethod
    def get_status_emoji(score):
        """Restituisce emoji basato sul punteggio"""
        if score >= 0.8:
            return "ğŸŸ¢"  # Eccellente
        elif score >= 0.6:
            return "âœ…"  # Buono
        elif score >= 0.4:
            return "âš ï¸"  # Discreto
        else:
            return "âŒ"  # Scarso

    @staticmethod
    def display_comprehensive_results(ragas_results, custom_results):
        """Visualizza risultati con debug esteso"""
        print("\n" + "="*80)
        print("ğŸ“Š RISULTATI VALUTAZIONE RAG COMPLETA")
        print("="*80)

        # Categorizza metriche RAGAS
        metric_categories = {
            "ğŸ¯ Core RAG Metrics": ["faithfulness", "answer_relevancy", "context_precision", "context_recall"],
            "ğŸ“ Answer Quality": ["answer_correctness", "answer_similarity"],
            "ğŸ“„ Context Analysis": ["context_entity_recall"],
            "ğŸ—£ï¸ Language Quality": ["coherence", "fluency", "conciseness"],
            "ğŸ›¡ï¸ Safety & Ethics": ["harmfulness", "maliciousness"],
            "ğŸ”¬ Specialized": ["summarization_score", "aspect_critique"]
        }

        # Mostra metriche RAGAS per categoria
        ragas_scores = []
        zero_scores = []

        print(f"\nğŸ“Š SEZIONE RAGAS METRICS:")
        if ragas_results and len(ragas_results) > 0:
            for category_name, metrics in metric_categories.items():
                category_metrics = {k: v for k,
                                    v in ragas_results.items() if k in metrics}
                if category_metrics:
                    print(f"\n{category_name}:")
                    for metric, value in category_metrics.items():
                        if not np.isnan(value):
                            status = DisplayHelper.get_status_emoji(value)
                            print(f"  {status} {metric:20}: {value:.4f}")
                            ragas_scores.append(value)
                            if value == 0:
                                zero_scores.append(metric)
                        else:
                            print(f"  ğŸ”¶ {metric:20}: NaN")
        else:
            print("  âŒ Nessuna metrica RAGAS disponibile")

        # Mostra metriche custom
        print(f"\nğŸ”§ SEZIONE CUSTOM METRICS:")
        custom_scores = []
        if custom_results and isinstance(custom_results, dict) and len(custom_results) > 0:
            print(f"âœ… Mostrando {len(custom_results)} metriche custom:")
            for metric, value in custom_results.items():
                try:
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        status = DisplayHelper.get_status_emoji(value)
                        print(f"  {status} {metric:25}: {value:.4f}")
                        custom_scores.append(value)
                    else:
                        print(f"  âš ï¸ {metric:25}: {value} (invalid)")
                except Exception as e:
                    print(f"  âŒ Errore processando {metric}: {e}")
        else:
            print("  âŒ Nessuna metrica custom disponibile")

        # Statistiche globali
        all_scores = ragas_scores + custom_scores

        print(f"\nğŸ“ˆ STATISTICHE GLOBALI:")
        print(f"  ğŸ“Š RAGAS scores: {len(ragas_scores)}")
        print(f"  ğŸ”§ Custom scores: {len(custom_scores)}")
        print(f"  ğŸ“‹ Total scores: {len(all_scores)}")

        if all_scores:
            avg_score = np.mean(all_scores)
            median_score = np.median(all_scores)
            max_score = np.max(all_scores)
            min_score = np.min(all_scores)

            print(f"  ğŸ“Š Score medio:      {avg_score:.4f}")
            print(f"  ğŸ¯ Score mediano:    {median_score:.4f}")
            print(f"  ğŸ† Score massimo:    {max_score:.4f}")
            print(f"  âš ï¸ Score minimo:     {min_score:.4f}")

            # Rating globale
            if avg_score >= 0.8:
                rating = "ğŸ† ECCELLENTE"
            elif avg_score >= 0.6:
                rating = "âœ… BUONO"
            elif avg_score >= 0.4:
                rating = "âš ï¸ DISCRETO"
            else:
                rating = "âŒ NECESSITA MIGLIORAMENTI"

            print(f"  ğŸ–ï¸ Rating globale:   {rating}")

            # Aree di miglioramento
            if zero_scores:
                print(f"\nğŸ”§ AREE DI MIGLIORAMENTO:")
                for metric in zero_scores:
                    print(f"  âŒ {metric}")
        else:
            print("  âŒ Nessun punteggio disponibile per le statistiche")

        print("\n" + "="*80)

    @staticmethod
    def format_comprehensive_results(ragas_results, custom_results):
        """Formatta risultati con debug esteso e restituisce una stringa"""
        output = []

        output.append("=" * 80)
        output.append("ğŸ“Š RISULTATI VALUTAZIONE RAG COMPLETA")
        output.append("=" * 80)

        # Categorizza metriche RAGAS
        metric_categories = {
            "ğŸ¯ Core RAG Metrics": ["faithfulness", "answer_relevancy", "context_precision", "context_recall"],
            "ğŸ“ Answer Quality": ["answer_correctness", "answer_similarity"],
            "ğŸ“„ Context Analysis": ["context_entity_recall"],
            "ğŸ—£ï¸ Language Quality": ["coherence", "fluency", "conciseness"],
            "ğŸ›¡ï¸ Safety & Ethics": ["harmfulness", "maliciousness"],
            "ğŸ”¬ Specialized": ["summarization_score", "aspect_critique"]
        }

        # Mostra metriche RAGAS per categoria
        ragas_scores = []
        zero_scores = []

        output.append("\nğŸ“Š SEZIONE RAGAS METRICS:")
        if ragas_results and len(ragas_results) > 0:
            for category_name, metrics in metric_categories.items():
                category_metrics = {
                    k: v for k, v in ragas_results.items() if k in metrics}
                if category_metrics:
                    output.append(f"\n{category_name}:")
                    for metric, value in category_metrics.items():
                        if not np.isnan(value):
                            status = DisplayHelper.get_status_emoji(value)
                            output.append(
                                f"  {status} {metric:20}: {value:.4f}")
                            ragas_scores.append(value)
                            if value == 0:
                                zero_scores.append(metric)
                        else:
                            output.append(f"  ğŸ”¶ {metric:20}: NaN")
        else:
            output.append("  âŒ Nessuna metrica RAGAS disponibile")

        # Mostra metriche custom
        output.append(f"\nğŸ”§ SEZIONE CUSTOM METRICS:")
        custom_scores = []
        if custom_results and isinstance(custom_results, dict) and len(custom_results) > 0:
            output.append(
                f"âœ… Mostrando {len(custom_results)} metriche custom:")
            for metric, value in custom_results.items():
                try:
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        status = DisplayHelper.get_status_emoji(value)
                        output.append(
                            f"  {status} {metric:25}: {value:.4f}")
                        custom_scores.append(value)
                    else:
                        output.append(
                            f"  âš ï¸ {metric:25}: {value} (invalid)")
                except Exception as e:
                    output.append(f"  âŒ Errore processando {metric}: {e}")
        else:
            output.append("  âŒ Nessuna metrica custom disponibile")

        # Statistiche globali
        all_scores = ragas_scores + custom_scores

        output.append(f"\nğŸ“ˆ STATISTICHE GLOBALI:")
        output.append(f"  ğŸ“Š RAGAS scores: {len(ragas_scores)}")
        output.append(f"  ğŸ”§ Custom scores: {len(custom_scores)}")
        output.append(f"  ğŸ“‹ Total scores: {len(all_scores)}")

        if all_scores:
            avg_score = np.mean(all_scores)
            median_score = np.median(all_scores)
            max_score = np.max(all_scores)
            min_score = np.min(all_scores)

            output.append(f"  ğŸ“Š Score medio:      {avg_score:.4f}")
            output.append(f"  ğŸ¯ Score mediano:    {median_score:.4f}")
            output.append(f"  ğŸ† Score massimo:    {max_score:.4f}")
            output.append(f"  âš ï¸ Score minimo:     {min_score:.4f}")

            # Rating globale
            if avg_score >= 0.8:
                rating = "ğŸ† ECCELLENTE"
            elif avg_score >= 0.6:
                rating = "âœ… BUONO"
            elif avg_score >= 0.4:
                rating = "âš ï¸ DISCRETO"
            else:
                rating = "âŒ NECESSITA MIGLIORAMENTI"

            output.append(f"  ğŸ–ï¸ Rating globale:   {rating}")

            # Aree di miglioramento
            if zero_scores:
                output.append(f"\nğŸ”§ AREE DI MIGLIORAMENTO:")
                for metric in zero_scores:
                    output.append(f"  âŒ {metric}")
        else:
            output.append(
                "  âŒ Nessun punteggio disponibile per le statistiche")

        output.append("\n" + "=" * 80)

        return "\n".join(output)

    @staticmethod
    def format_comprehensive_results_simple(ragas_results, custom_results):
        """Versione semplificata che restituisce solo le statistiche principali"""
        ragas_scores = [v for v in ragas_results.values() if isinstance(
            v, (int, float)) and not np.isnan(v)] if ragas_results else []
        custom_scores = [v for v in custom_results.values() if isinstance(
            v, (int, float)) and not np.isnan(v)] if custom_results else []
        all_scores = ragas_scores + custom_scores

        if not all_scores:
            return "âŒ Nessun punteggio disponibile"

        avg_score = np.mean(all_scores)

        # Rating
        if avg_score >= 0.8:
            rating = "ğŸ† ECCELLENTE"
        elif avg_score >= 0.6:
            rating = "âœ… BUONO"
        elif avg_score >= 0.4:
            rating = "âš ï¸ DISCRETO"
        else:
            rating = "âŒ NECESSITA MIGLIORAMENTI"

        return f"ğŸ“Š Score: {avg_score:.4f} | {rating} | RAGAS: {len(ragas_scores)} | Custom: {len(custom_scores)}"
